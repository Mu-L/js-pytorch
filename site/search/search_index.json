{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Js-Pytorch's documentation For access to the source code, visit The GitHub repo . About JS-PyTorch is a Deep Learning JavaScript library built from scratch, to closely follow PyTorch's syntax. This means that you can use this library to train, test and deploy Neural Networks, with node.js or on a web browser. Installation This is a node package, and can be installed with npm (Node Package Manager). It has full sopport of node 20.15.1, which is the latest LTS (Long-Term Support) node version. In most operating systems, it should also work for more recent versions. MacOS First, install node with the command line, as described on the node website : # installs nvm (Node Version Manager) curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash # download and install Node.js (you may need to restart the terminal) nvm install 20 # verifies the right Node.js version is in the environment node -v # should print `v20.15.1` # verifies the right npm version is in the environment npm -v # should print `10.7.0` Now, use npm to install Js-PyTorch locally: # installs js-pytorch npm install js-pytorch # if needed, install older version of js-pytorch nvm install js-pytorch@0.1.0 Finally, require the package in your javascript file: const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; Linux First, install node with the command line, as described on the node website : # installs nvm (Node Version Manager) curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash # download and install Node.js (you may need to restart the terminal) nvm install 20 # verifies the right Node.js version is in the environment node -v # should print `v20.15.1` # verifies the right npm version is in the environment npm -v # should print `10.7.0` Now, use npm to install Js-PyTorch locally: # installs js-pytorch npm install js-pytorch # if needed, install older version of js-pytorch nvm install js-pytorch@0.1.0 Finally, require the package in your javascript file: const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; Windows First, download node from the prebuilt installer on the node website : Now, use npm to install Js-PyTorch locally: # installs js-pytorch npm install js-pytorch # if needed, install older version of js-pytorch nvm install js-pytorch@0.1.0 Note: If this throws an error, you might need to install the latest version of Visual Studio , including the \"Desktop development with C++\" workload. Finally, require the package in your javascript file: const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; Contributing If you have detected a bug on the library, please file a Bug Report using a GitHub issue, and feel free to reach out to me on my LinkedIn or email. If you would like to see a new feature in Js-PyTorch, file a New Feature issue. Finally, if you would like to contribute, create a merge request to the develop branch. I will try to answer as soon as possible. All help is really appreciated! Here is a list of the developer tools : Build for Distribution by running npm run build . CJS and ESM modules and index.d.ts will be output in the dist/ folder. Check the Code with ESLint at any time, running npm run lint . Run tests run npm test . Improve Code Formatting with prettier, running npm run prettier . Performance Benchmarks are also included in the tests/benchmarks/ directory. Run all benchmarks with npm run bench and save new benchmarks with npm run bench:update .","title":"Home"},{"location":"#welcome-to-js-pytorchs-documentation","text":"For access to the source code, visit The GitHub repo .","title":"Welcome to Js-Pytorch's documentation"},{"location":"#about","text":"JS-PyTorch is a Deep Learning JavaScript library built from scratch, to closely follow PyTorch's syntax. This means that you can use this library to train, test and deploy Neural Networks, with node.js or on a web browser.","title":"About"},{"location":"#installation","text":"This is a node package, and can be installed with npm (Node Package Manager). It has full sopport of node 20.15.1, which is the latest LTS (Long-Term Support) node version. In most operating systems, it should also work for more recent versions.","title":"Installation"},{"location":"#macos","text":"First, install node with the command line, as described on the node website : # installs nvm (Node Version Manager) curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash # download and install Node.js (you may need to restart the terminal) nvm install 20 # verifies the right Node.js version is in the environment node -v # should print `v20.15.1` # verifies the right npm version is in the environment npm -v # should print `10.7.0` Now, use npm to install Js-PyTorch locally: # installs js-pytorch npm install js-pytorch # if needed, install older version of js-pytorch nvm install js-pytorch@0.1.0 Finally, require the package in your javascript file: const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim;","title":"MacOS"},{"location":"#linux","text":"First, install node with the command line, as described on the node website : # installs nvm (Node Version Manager) curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash # download and install Node.js (you may need to restart the terminal) nvm install 20 # verifies the right Node.js version is in the environment node -v # should print `v20.15.1` # verifies the right npm version is in the environment npm -v # should print `10.7.0` Now, use npm to install Js-PyTorch locally: # installs js-pytorch npm install js-pytorch # if needed, install older version of js-pytorch nvm install js-pytorch@0.1.0 Finally, require the package in your javascript file: const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim;","title":"Linux"},{"location":"#windows","text":"First, download node from the prebuilt installer on the node website : Now, use npm to install Js-PyTorch locally: # installs js-pytorch npm install js-pytorch # if needed, install older version of js-pytorch nvm install js-pytorch@0.1.0 Note: If this throws an error, you might need to install the latest version of Visual Studio , including the \"Desktop development with C++\" workload. Finally, require the package in your javascript file: const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim;","title":"Windows"},{"location":"#contributing","text":"If you have detected a bug on the library, please file a Bug Report using a GitHub issue, and feel free to reach out to me on my LinkedIn or email. If you would like to see a new feature in Js-PyTorch, file a New Feature issue. Finally, if you would like to contribute, create a merge request to the develop branch. I will try to answer as soon as possible. All help is really appreciated! Here is a list of the developer tools : Build for Distribution by running npm run build . CJS and ESM modules and index.d.ts will be output in the dist/ folder. Check the Code with ESLint at any time, running npm run lint . Run tests run npm test . Improve Code Formatting with prettier, running npm run prettier . Performance Benchmarks are also included in the tests/benchmarks/ directory. Run all benchmarks with npm run bench and save new benchmarks with npm run bench:update .","title":"Contributing"},{"location":"layers/","text":"Layers In this section are listed all of the Layers and Modules . nn.Linear new nn.Linear(in_size, out_size, device, bias, xavier) \u2192 Tensor Applies a linear transformation to the input tensor. Input is matrix-multiplied by a w tensor and added to a b tensor. Parameters in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' . bias (boolean) - Whether to use a bias term b . xavier (boolean) - Whether to use Xavier initialization on the weights. Learnable Variables w - [input_size, output_size] Tensor. b - [output_size] Tensor. Example >>> let linear = new nn.Linear(10,15,'gpu'); >>> let x = torch.randn([100,50,10], true, 'gpu'); >>> let y = linear.forward(x); >>> y.shape // [100, 50, 15] nn.MultiHeadSelfAttention new nn.MultiHeadSelfAttention(in_size, out_size, n_heads, n_timesteps, dropout_prob, device) \u2192 Tensor Applies a self-attention layer on the input tensor. Matrix-multiplies input by Wk , Wq , Wv , resulting in Key, Query and Value tensors. Computes attention multiplying Query and transpose Key. Applies Mask, Dropout and Softmax to attention activations. Multiplies result by Values. Multiplies result by residual_proj . Applies final Dropout. Parameters in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. n_heads (boolean) - Number of parallel attention heads the data is divided into. In_size must be divided evenly by n_heads. n_timesteps (boolean) - Number of timesteps computed in parallel by the transformer. dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization). device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' . Learnable Variables Wk - [input_size, input_size] Tensor. Wq - [input_size, input_size] Tensor. Wv - [input_size, input_size] Tensor. residual_proj - [input_size, output_size] Tensor. Example >>> let att = new nn.MultiHeadSelfAttention(10, 15, 2, 32, 0.2, 'gpu'); >>> let x = torch.randn([100,50,10], true, 'gpu'); >>> let y = att.forward(x); >>> y.shape // [100, 50, 15] nn.FullyConnected new nn.FullyConnected(in_size, out_size, dropout_prob, device, bias) \u2192 Tensor Applies a fully-connected layer on the input tensor. Matrix-multiplies input by Linear layer l1 , upscaling the input. Passes tensor through ReLU. Matrix-multiplies tensor by Linear layer l2 , downscaling the input. Passes tensor through Dropout. forward(x: Tensor): Tensor { let z = this.l1.forward(x); z = this.relu.forward(z); z = this.l2.forward(z); z = this.dropout.forward(z); return z; } Parameters in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization). device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' . bias (boolean) - Whether to use a bias term b . Learnable Variables l1 - [input_size, 4 input_size]* Tensor. l2 - [4 input_size, input_size]* Tensor. Example >>> let fc = new nn.FullyConnected(10, 15, 0.2, 'gpu'); >>> let x = torch.randn([100,50,10], true, 'gpu'); >>> let y = fc.forward(x); >>> y.shape // [100, 50, 15] nn.Block new nn.Block(in_size, out_size, n_heads, n_timesteps, dropout_prob, device) \u2192 Tensor Applies a transformer Block layer on the input tensor. forward(x: Tensor): Tensor { // Pass through Layer Norm and Self Attention: let z = x.add(this.att.forward(this.ln1.forward(x))); // Pass through Layer Norm and Fully Connected: z = z.add(this.fcc.forward(this.ln2.forward(z))); return z; } Parameters in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. n_heads (boolean) - Number of parallel attention heads the data is divided into. In_size must be divided evenly by n_heads. n_timesteps (boolean) - Number of timesteps computed in parallel by the transformer. dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization). device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' . Learnable Modules nn.MultiHeadSelfAttention - Wk , Wq , Wv , residual_proj . nn.LayerNorm - gamma , beta . nn.FullyConnecyed - l1 , l2 . nn.LayerNorm - gamma , beta .","title":"Layers"},{"location":"layers/#layers","text":"In this section are listed all of the Layers and Modules .","title":"Layers"},{"location":"layers/#nnlinear","text":"new nn.Linear(in_size, out_size, device, bias, xavier) \u2192 Tensor Applies a linear transformation to the input tensor. Input is matrix-multiplied by a w tensor and added to a b tensor.","title":"nn.Linear"},{"location":"layers/#parameters","text":"in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' . bias (boolean) - Whether to use a bias term b . xavier (boolean) - Whether to use Xavier initialization on the weights.","title":"Parameters"},{"location":"layers/#learnable-variables","text":"w - [input_size, output_size] Tensor. b - [output_size] Tensor.","title":"Learnable Variables"},{"location":"layers/#example","text":">>> let linear = new nn.Linear(10,15,'gpu'); >>> let x = torch.randn([100,50,10], true, 'gpu'); >>> let y = linear.forward(x); >>> y.shape // [100, 50, 15]","title":"Example"},{"location":"layers/#nnmultiheadselfattention","text":"new nn.MultiHeadSelfAttention(in_size, out_size, n_heads, n_timesteps, dropout_prob, device) \u2192 Tensor Applies a self-attention layer on the input tensor. Matrix-multiplies input by Wk , Wq , Wv , resulting in Key, Query and Value tensors. Computes attention multiplying Query and transpose Key. Applies Mask, Dropout and Softmax to attention activations. Multiplies result by Values. Multiplies result by residual_proj . Applies final Dropout.","title":"nn.MultiHeadSelfAttention"},{"location":"layers/#parameters_1","text":"in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. n_heads (boolean) - Number of parallel attention heads the data is divided into. In_size must be divided evenly by n_heads. n_timesteps (boolean) - Number of timesteps computed in parallel by the transformer. dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization). device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' .","title":"Parameters"},{"location":"layers/#learnable-variables_1","text":"Wk - [input_size, input_size] Tensor. Wq - [input_size, input_size] Tensor. Wv - [input_size, input_size] Tensor. residual_proj - [input_size, output_size] Tensor.","title":"Learnable Variables"},{"location":"layers/#example_1","text":">>> let att = new nn.MultiHeadSelfAttention(10, 15, 2, 32, 0.2, 'gpu'); >>> let x = torch.randn([100,50,10], true, 'gpu'); >>> let y = att.forward(x); >>> y.shape // [100, 50, 15]","title":"Example"},{"location":"layers/#nnfullyconnected","text":"new nn.FullyConnected(in_size, out_size, dropout_prob, device, bias) \u2192 Tensor Applies a fully-connected layer on the input tensor. Matrix-multiplies input by Linear layer l1 , upscaling the input. Passes tensor through ReLU. Matrix-multiplies tensor by Linear layer l2 , downscaling the input. Passes tensor through Dropout. forward(x: Tensor): Tensor { let z = this.l1.forward(x); z = this.relu.forward(z); z = this.l2.forward(z); z = this.dropout.forward(z); return z; }","title":"nn.FullyConnected"},{"location":"layers/#parameters_2","text":"in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization). device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' . bias (boolean) - Whether to use a bias term b .","title":"Parameters"},{"location":"layers/#learnable-variables_2","text":"l1 - [input_size, 4 input_size]* Tensor. l2 - [4 input_size, input_size]* Tensor.","title":"Learnable Variables"},{"location":"layers/#example_2","text":">>> let fc = new nn.FullyConnected(10, 15, 0.2, 'gpu'); >>> let x = torch.randn([100,50,10], true, 'gpu'); >>> let y = fc.forward(x); >>> y.shape // [100, 50, 15]","title":"Example"},{"location":"layers/#nnblock","text":"new nn.Block(in_size, out_size, n_heads, n_timesteps, dropout_prob, device) \u2192 Tensor Applies a transformer Block layer on the input tensor. forward(x: Tensor): Tensor { // Pass through Layer Norm and Self Attention: let z = x.add(this.att.forward(this.ln1.forward(x))); // Pass through Layer Norm and Fully Connected: z = z.add(this.fcc.forward(this.ln2.forward(z))); return z; }","title":"nn.Block"},{"location":"layers/#parameters_3","text":"in_size (number) - Size of the last dimension of the input data. out_size (number) - Size of the last dimension of the output data. n_heads (boolean) - Number of parallel attention heads the data is divided into. In_size must be divided evenly by n_heads. n_timesteps (boolean) - Number of timesteps computed in parallel by the transformer. dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization). device (string) - Device on which the model's calculations will run. Either 'cpu' or 'gpu' .","title":"Parameters"},{"location":"layers/#learnable-modules","text":"nn.MultiHeadSelfAttention - Wk , Wq , Wv , residual_proj . nn.LayerNorm - gamma , beta . nn.FullyConnecyed - l1 , l2 . nn.LayerNorm - gamma , beta .","title":"Learnable Modules"},{"location":"operations/","text":"Tensor Operations In this section are listed all of the Tensor Operation methods. torch.add torch.add(a, b) \u2192 Tensor If both tensors are scalars, the simple sum is returned. If one tensor is a scalar, the element-wise sum of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and add them element-wise. Parameters a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number. Example >>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >>> let b = torch.tensor([1]); >>> let c = torch.add(a,b); >>> c.data; //[[2,2,3,4], // [7,8,9,10]] >>> b = torch.tensor([[0], [2]]); >>> c = torch.add(a,b); >>> c.data; //[[1,1,2,3], // [8,9,10,11]] >>> b = torch.tensor([[0,0,0,0], [0,0,100,0]]); >>> c = torch.add(a,b); >>> c.data; //[[1,1,2,3], // [6,7,108,9]] Note: torch.add(a, b) is the same as a.add(b) . torch.sub torch.sub(a, b) \u2192 Tensor If both tensors are scalars, the simple subtraction is returned. If one tensor is a scalar, the element-wise subtraction of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and subtract them element-wise. Parameters a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number. Example >>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >>> let b = torch.tensor([1]); >>> let c = torch.sub(a,b); >>> c.data; //[[0,0,1,2], // [5,6,7,8]] >>> b = torch.tensor([[0], [2]]); >>> c = torch.sub(a,b); >>> c.data; //[[1,1,2,3], // [4,5,6,7]] >>> b = torch.tensor([[0,0,0,0], [0,0,8,0]]); >>> c = torch.sub(a,b); >>> c.data; //[[1,1,2,3], // [6,7,0,9]] Note: torch.sub(a, b) is the same as a.sub(b) . torch.neg torch.neg(a) \u2192 Tensor Returns the element-wise opposite of the given Tensor. Parameters a (Tensor | number) - Input Tensor or number. Example >>> let a = torch.tensor([1]); >>> let b = torch.neg(a); >>> c.data; // [-1] >>> a = torch.tensor([-3]); >>> b = torch.neg(a); >>> c.data; // [3] >>> a = torch.tensor([[0,1,0,-1], [-3,2,1,0]]); >>> b = torch.neg(a); >>> c.data; //[[0,-1,0,1], // [3,-2,-1,0]] Note: torch.neg(a) is the same as a.neg() . torch.mul torch.mul(a, b) \u2192 Tensor If both tensors are scalars, the simple dot product is returned. If one tensor is a scalar, the element-wise product of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and multiply them element-wise. Parameters a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number. Example >>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >>> let b = torch.tensor([2]); >>> let c = torch.mul(a,b); >>> c.data; //[[0,0,1,2], // [5,6,7,8]] >>> b = torch.tensor([[0], [-1]]); >>> c = torch.mul(a,b); >>> c.data; //[[0, 0, 0, 0], // [-6,-7,-8,-9]] >>> b = torch.tensor([[0,0,0,0], [0,0,8,0]]); >>> c = torch.mul(a,b); >>> c.data; //[[1,1,2,3], // [6,7,0,9]] Note: torch.mul(a, b) is the same as a.mul(b) . torch.div torch.div(a, b) \u2192 Tensor If both tensors are scalars, the simple division is returned. If one tensor is a scalar, the element-wise division of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and divide them element-wise. Parameters a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number. Example >>> let a = torch.tensor([[2,-2,4,6], [6,-6,8,8]]); >>> let b = torch.tensor([2]); >>> let c = torch.div(a,b); >>> c.data; //[[1,-1,2,3], // [3,-3,4,4]] >>> b = torch.tensor([[1], [-1]]); >>> c = torch.div(a,b); >>> c.data; //[[2,-2, 4, 6], // [-6,6,-8,-8]] >>> b = torch.tensor([[1,1,1,1], [1,1,16,1]]); >>> c = torch.div(a,b); >>> c.data; //[[2,-2, 4, 6], // [6,-6,0.5,8]] Note: torch.div(a, b) is the same as a.div(b) . torch.matmul torch.matlul(a, b) \u2192 Tensor Performs matrix multiplication between the last two dimensions of each Tensor. If inputs are of shape [H,W] and [W,C] , the output will have shape [H,C] . If the two Tensors have more than two dimensions, they can be broadcast : [B,N,H,W], [B,N,W,C] => [B,N,H,C] [B,N,H,W], [W,C] => [B,N,H,C] [H,W], [B,N,W,C] => [B,N,H,C] [B,N,H,W], [1,1,W,C] => [B,N,H,C] Parameters a (Tensor | number) - Input Tensor. b (Tensor | number) - Other Tensor. Example >>> let a = torch.tensor([[1,1,1,2], [3,1,0,0]]); // Shape [2,4] >>> let b = torch.tensor([[1], [0], [0], [0]]); // Shape [4,1] >>> let c = torch.matmul(a,b); // Shape [2,1] >>> c.data; //[[1], // [3]] Note: torch.matmul(a, b) is the same as a.matmul(b) . torch.sum torch.sum(a, dim, keepdims=false) \u2192 Tensor Gets the sum of the Tensor over a specified dimension. Parameters a (Tensor) - Input Tensor. dim (integer) - Dimension to perform the sum over. keepdims (boolean) - Whether to keep dimensions of original tensor. Example >>> let a = torch.ones([4,3], false, 'gpu'); >>> a.data; //[[1, 1, 1], // [1, 1, 1] // [1, 1, 1] // [1, 1, 1]] >>> let b = torch.sum(a, 0); >>> b.data; // [[4, 4, 4]] >>> b = torch.sum(a, 1); >>> b.data; // [[3], // [3], // [3], // [3]] >>> b = torch.sum(a, 0, true); >>> b.data; //[[4, 4, 4], // [4, 4, 4] // [4, 4, 4] // [4, 4, 4]] Note: torch.sum(a) is the same as a.sum() . torch.mean torch.mean(a, dim, keepdims=false) \u2192 Tensor Gets the mean of the Tensor over a specified dimension. Parameters a (Tensor) - Input Tensor. dim (integer) - Dimension to get the mean of. keepdims (boolean) - Whether to keep dimensions of original tensor. Example >>> let a = torch.randint(0, 2, [2,3], false, 'gpu'); >>> a.data; //[[0, 1, 0], // [1, 1, 1]] >>> let b = torch.mean(a, 0); >>> b.data; // [[0.5, 1, 0.5]] >>> b = torch.mean(a, 1); >>> b.data; // [[0.333333], // [1]] >>> b = torch.mean(a, 0, true); >>> b.data; //[[0.5, 1, 0.5], // [0.5, 1, 0.5]] Note: torch.mean(a) is the same as a.mean() . torch.variance torch.variance(a, dim, keepdims=false) \u2192 Tensor Gets the variance of the Tensor over a specified dimension. Parameters a (Tensor) - Input Tensor. dim (integer) - Dimension to get the variance of. keepdims (boolean) - Whether to keep dimensions of original tensor. Example >>> let a = torch.randint(0, 3, [3,2], false, 'gpu'); >>> a.data; //[[0, 2], // [2, 1], // [0, 1]] >>> let b = torch.variance(a, 0); >>> b.data; // [[0.9428, 0.471404]] >>> b = torch.variance(a, 0, true); >>> b.data; //[[0.9428, 0.471404] // [0.9428, 0.471404] // [0.9428, 0.471404]] Note: torch.variance(a) is the same as a.variance() . torch.transpose torch.transpose(a, dim1, dim2) \u2192 Tensor Transposes the tensor along two consecutive dimensions. Parameters a (Tensor) - Input Tensor. dim1 (integer) - First dimension. dim2 (boolean) - Second dimension. Example >>> let a = torch.randint(0, 3, [3,2], false, 'gpu'); >>> a.data; //[[0, 2], // [2, 1], // [0, 1]] >>> let b = torch.transpose(a, -1, -2); >>> b.data; //[[0, 2, 0], // [2, 1, 1]] Note: torch.transpose(a) is the same as a.transpose() . torch.at torch.at(a, dim1, dim2) \u2192 Tensor If a single Array index1 is passed, returns the elements in the tensor indexed by this Array: tensor[index1] . If a two Arrays index1 and index2 are passed, returns the elements in the tensor indexed by tensor[index1][index2] . Parameters a (Tensor) - Input Tensor. index1 (Array) - Array containing indexes to extract data from in first dimension. index2 (Array) - Array containing indexes to extract data from in second dimension. Example >>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >> let b = torch.at(a,[0,1,1], [2,0,3]); >>> b.data; // [2,6,9] >>> b = torch.at(a,[0,1,0]); >>> b.data; // [[1,1,2,3], // [6,7,8,9], // [1,1,2,3]]) Note: torch.at(a) is the same as a.at() . torch.masked_fill torch.masked_fill(a, condition, value) \u2192 Tensor A condition function scans the a tensor element-wise, returning true or false . In places within the a tensor where the \"condition\" function returns True, we set the value to value . Parameters a (Tensor) - Input Tensor. condition (function) - Function that returns True or False element-wise. value (number) - Value to fill Tensor when condition is met. Example >>> let a = torch.tensor([[1,5,2,3], [6,7,2,9]]); >>> let b = torch.masked_fill(a, mask, (el) => {return el > 3}, 0); >>> b.data; // [[1,0,2,3], // [0,0,2,0]] Note: torch.masked_fill(a) is the same as a.masked_fill() . torch.pow torch.pow(a, n) \u2192 Tensor Returns tensor to element-wise power of n. Parameters a (Tensor) - Input Tensor. n (function) - Exponent. Example >>> let a = torch.tensor([[1,-5], [6,7]]); >>> let b = torch.pow(a, 2); >>> b.data; // [[1,25], // [36,49]] Note: torch.pow(a) is the same as a.pow() . torch.sqrt torch.sqrt(a) \u2192 Tensor Returns element-wise square root of the tensor. Parameters a (Tensor) - Input Tensor. Example >>> let a = torch.tensor([[1,9], [4,16]]); >>> let b = torch.sqrt(a); >>> b.data; // [[1,3], // [2,4]] Note: torch.sqrt(a) is the same as a.sqrt() . torch.exp torch.exp(a) \u2192 Tensor Returns element-wise exponentiation of the tensor. Parameters a (Tensor) - Input Tensor. Example >>> let a = torch.tensor([[1,2], [0,-1]]); >>> let b = torch.exp(a); >>> b.data; // [[2.71828,7.389056], // [1.00000,0.36788]] Note: torch.exp(a) is the same as a.exp() . torch.log torch.log(a) \u2192 Tensor Returns element-wise natural log of the tensor. Parameters a (Tensor) - Input Tensor. Example >>> let a = torch.tensor([[1,2], [0.01,3]]); >>> let b = torch.log(a); >>> b.data; // [[0.00000,0.693147], // [-4.6051,1.098612]] Note: torch.log(a) is the same as a.log() .","title":"Operations"},{"location":"operations/#tensor-operations","text":"In this section are listed all of the Tensor Operation methods.","title":"Tensor Operations"},{"location":"operations/#torchadd","text":"torch.add(a, b) \u2192 Tensor If both tensors are scalars, the simple sum is returned. If one tensor is a scalar, the element-wise sum of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and add them element-wise.","title":"torch.add"},{"location":"operations/#parameters","text":"a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number.","title":"Parameters"},{"location":"operations/#example","text":">>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >>> let b = torch.tensor([1]); >>> let c = torch.add(a,b); >>> c.data; //[[2,2,3,4], // [7,8,9,10]] >>> b = torch.tensor([[0], [2]]); >>> c = torch.add(a,b); >>> c.data; //[[1,1,2,3], // [8,9,10,11]] >>> b = torch.tensor([[0,0,0,0], [0,0,100,0]]); >>> c = torch.add(a,b); >>> c.data; //[[1,1,2,3], // [6,7,108,9]] Note: torch.add(a, b) is the same as a.add(b) .","title":"Example"},{"location":"operations/#torchsub","text":"torch.sub(a, b) \u2192 Tensor If both tensors are scalars, the simple subtraction is returned. If one tensor is a scalar, the element-wise subtraction of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and subtract them element-wise.","title":"torch.sub"},{"location":"operations/#parameters_1","text":"a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number.","title":"Parameters"},{"location":"operations/#example_1","text":">>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >>> let b = torch.tensor([1]); >>> let c = torch.sub(a,b); >>> c.data; //[[0,0,1,2], // [5,6,7,8]] >>> b = torch.tensor([[0], [2]]); >>> c = torch.sub(a,b); >>> c.data; //[[1,1,2,3], // [4,5,6,7]] >>> b = torch.tensor([[0,0,0,0], [0,0,8,0]]); >>> c = torch.sub(a,b); >>> c.data; //[[1,1,2,3], // [6,7,0,9]] Note: torch.sub(a, b) is the same as a.sub(b) .","title":"Example"},{"location":"operations/#torchneg","text":"torch.neg(a) \u2192 Tensor Returns the element-wise opposite of the given Tensor.","title":"torch.neg"},{"location":"operations/#parameters_2","text":"a (Tensor | number) - Input Tensor or number.","title":"Parameters"},{"location":"operations/#example_2","text":">>> let a = torch.tensor([1]); >>> let b = torch.neg(a); >>> c.data; // [-1] >>> a = torch.tensor([-3]); >>> b = torch.neg(a); >>> c.data; // [3] >>> a = torch.tensor([[0,1,0,-1], [-3,2,1,0]]); >>> b = torch.neg(a); >>> c.data; //[[0,-1,0,1], // [3,-2,-1,0]] Note: torch.neg(a) is the same as a.neg() .","title":"Example"},{"location":"operations/#torchmul","text":"torch.mul(a, b) \u2192 Tensor If both tensors are scalars, the simple dot product is returned. If one tensor is a scalar, the element-wise product of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and multiply them element-wise.","title":"torch.mul"},{"location":"operations/#parameters_3","text":"a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number.","title":"Parameters"},{"location":"operations/#example_3","text":">>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >>> let b = torch.tensor([2]); >>> let c = torch.mul(a,b); >>> c.data; //[[0,0,1,2], // [5,6,7,8]] >>> b = torch.tensor([[0], [-1]]); >>> c = torch.mul(a,b); >>> c.data; //[[0, 0, 0, 0], // [-6,-7,-8,-9]] >>> b = torch.tensor([[0,0,0,0], [0,0,8,0]]); >>> c = torch.mul(a,b); >>> c.data; //[[1,1,2,3], // [6,7,0,9]] Note: torch.mul(a, b) is the same as a.mul(b) .","title":"Example"},{"location":"operations/#torchdiv","text":"torch.div(a, b) \u2192 Tensor If both tensors are scalars, the simple division is returned. If one tensor is a scalar, the element-wise division of the scalar and the tensor is returned. If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and divide them element-wise.","title":"torch.div"},{"location":"operations/#parameters_4","text":"a (Tensor | number) - Input Tensor or number. b (Tensor | number) - Other Tensor or number.","title":"Parameters"},{"location":"operations/#example_4","text":">>> let a = torch.tensor([[2,-2,4,6], [6,-6,8,8]]); >>> let b = torch.tensor([2]); >>> let c = torch.div(a,b); >>> c.data; //[[1,-1,2,3], // [3,-3,4,4]] >>> b = torch.tensor([[1], [-1]]); >>> c = torch.div(a,b); >>> c.data; //[[2,-2, 4, 6], // [-6,6,-8,-8]] >>> b = torch.tensor([[1,1,1,1], [1,1,16,1]]); >>> c = torch.div(a,b); >>> c.data; //[[2,-2, 4, 6], // [6,-6,0.5,8]] Note: torch.div(a, b) is the same as a.div(b) .","title":"Example"},{"location":"operations/#torchmatmul","text":"torch.matlul(a, b) \u2192 Tensor Performs matrix multiplication between the last two dimensions of each Tensor. If inputs are of shape [H,W] and [W,C] , the output will have shape [H,C] . If the two Tensors have more than two dimensions, they can be broadcast : [B,N,H,W], [B,N,W,C] => [B,N,H,C] [B,N,H,W], [W,C] => [B,N,H,C] [H,W], [B,N,W,C] => [B,N,H,C] [B,N,H,W], [1,1,W,C] => [B,N,H,C]","title":"torch.matmul"},{"location":"operations/#parameters_5","text":"a (Tensor | number) - Input Tensor. b (Tensor | number) - Other Tensor.","title":"Parameters"},{"location":"operations/#example_5","text":">>> let a = torch.tensor([[1,1,1,2], [3,1,0,0]]); // Shape [2,4] >>> let b = torch.tensor([[1], [0], [0], [0]]); // Shape [4,1] >>> let c = torch.matmul(a,b); // Shape [2,1] >>> c.data; //[[1], // [3]] Note: torch.matmul(a, b) is the same as a.matmul(b) .","title":"Example"},{"location":"operations/#torchsum","text":"torch.sum(a, dim, keepdims=false) \u2192 Tensor Gets the sum of the Tensor over a specified dimension.","title":"torch.sum"},{"location":"operations/#parameters_6","text":"a (Tensor) - Input Tensor. dim (integer) - Dimension to perform the sum over. keepdims (boolean) - Whether to keep dimensions of original tensor.","title":"Parameters"},{"location":"operations/#example_6","text":">>> let a = torch.ones([4,3], false, 'gpu'); >>> a.data; //[[1, 1, 1], // [1, 1, 1] // [1, 1, 1] // [1, 1, 1]] >>> let b = torch.sum(a, 0); >>> b.data; // [[4, 4, 4]] >>> b = torch.sum(a, 1); >>> b.data; // [[3], // [3], // [3], // [3]] >>> b = torch.sum(a, 0, true); >>> b.data; //[[4, 4, 4], // [4, 4, 4] // [4, 4, 4] // [4, 4, 4]] Note: torch.sum(a) is the same as a.sum() .","title":"Example"},{"location":"operations/#torchmean","text":"torch.mean(a, dim, keepdims=false) \u2192 Tensor Gets the mean of the Tensor over a specified dimension.","title":"torch.mean"},{"location":"operations/#parameters_7","text":"a (Tensor) - Input Tensor. dim (integer) - Dimension to get the mean of. keepdims (boolean) - Whether to keep dimensions of original tensor.","title":"Parameters"},{"location":"operations/#example_7","text":">>> let a = torch.randint(0, 2, [2,3], false, 'gpu'); >>> a.data; //[[0, 1, 0], // [1, 1, 1]] >>> let b = torch.mean(a, 0); >>> b.data; // [[0.5, 1, 0.5]] >>> b = torch.mean(a, 1); >>> b.data; // [[0.333333], // [1]] >>> b = torch.mean(a, 0, true); >>> b.data; //[[0.5, 1, 0.5], // [0.5, 1, 0.5]] Note: torch.mean(a) is the same as a.mean() .","title":"Example"},{"location":"operations/#torchvariance","text":"torch.variance(a, dim, keepdims=false) \u2192 Tensor Gets the variance of the Tensor over a specified dimension.","title":"torch.variance"},{"location":"operations/#parameters_8","text":"a (Tensor) - Input Tensor. dim (integer) - Dimension to get the variance of. keepdims (boolean) - Whether to keep dimensions of original tensor.","title":"Parameters"},{"location":"operations/#example_8","text":">>> let a = torch.randint(0, 3, [3,2], false, 'gpu'); >>> a.data; //[[0, 2], // [2, 1], // [0, 1]] >>> let b = torch.variance(a, 0); >>> b.data; // [[0.9428, 0.471404]] >>> b = torch.variance(a, 0, true); >>> b.data; //[[0.9428, 0.471404] // [0.9428, 0.471404] // [0.9428, 0.471404]] Note: torch.variance(a) is the same as a.variance() .","title":"Example"},{"location":"operations/#torchtranspose","text":"torch.transpose(a, dim1, dim2) \u2192 Tensor Transposes the tensor along two consecutive dimensions.","title":"torch.transpose"},{"location":"operations/#parameters_9","text":"a (Tensor) - Input Tensor. dim1 (integer) - First dimension. dim2 (boolean) - Second dimension.","title":"Parameters"},{"location":"operations/#example_9","text":">>> let a = torch.randint(0, 3, [3,2], false, 'gpu'); >>> a.data; //[[0, 2], // [2, 1], // [0, 1]] >>> let b = torch.transpose(a, -1, -2); >>> b.data; //[[0, 2, 0], // [2, 1, 1]] Note: torch.transpose(a) is the same as a.transpose() .","title":"Example"},{"location":"operations/#torchat","text":"torch.at(a, dim1, dim2) \u2192 Tensor If a single Array index1 is passed, returns the elements in the tensor indexed by this Array: tensor[index1] . If a two Arrays index1 and index2 are passed, returns the elements in the tensor indexed by tensor[index1][index2] .","title":"torch.at"},{"location":"operations/#parameters_10","text":"a (Tensor) - Input Tensor. index1 (Array) - Array containing indexes to extract data from in first dimension. index2 (Array) - Array containing indexes to extract data from in second dimension.","title":"Parameters"},{"location":"operations/#example_10","text":">>> let a = torch.tensor([[1,1,2,3], [6,7,8,9]]); >> let b = torch.at(a,[0,1,1], [2,0,3]); >>> b.data; // [2,6,9] >>> b = torch.at(a,[0,1,0]); >>> b.data; // [[1,1,2,3], // [6,7,8,9], // [1,1,2,3]]) Note: torch.at(a) is the same as a.at() .","title":"Example"},{"location":"operations/#torchmasked_fill","text":"torch.masked_fill(a, condition, value) \u2192 Tensor A condition function scans the a tensor element-wise, returning true or false . In places within the a tensor where the \"condition\" function returns True, we set the value to value .","title":"torch.masked_fill"},{"location":"operations/#parameters_11","text":"a (Tensor) - Input Tensor. condition (function) - Function that returns True or False element-wise. value (number) - Value to fill Tensor when condition is met.","title":"Parameters"},{"location":"operations/#example_11","text":">>> let a = torch.tensor([[1,5,2,3], [6,7,2,9]]); >>> let b = torch.masked_fill(a, mask, (el) => {return el > 3}, 0); >>> b.data; // [[1,0,2,3], // [0,0,2,0]] Note: torch.masked_fill(a) is the same as a.masked_fill() .","title":"Example"},{"location":"operations/#torchpow","text":"torch.pow(a, n) \u2192 Tensor Returns tensor to element-wise power of n.","title":"torch.pow"},{"location":"operations/#parameters_12","text":"a (Tensor) - Input Tensor. n (function) - Exponent.","title":"Parameters"},{"location":"operations/#example_12","text":">>> let a = torch.tensor([[1,-5], [6,7]]); >>> let b = torch.pow(a, 2); >>> b.data; // [[1,25], // [36,49]] Note: torch.pow(a) is the same as a.pow() .","title":"Example"},{"location":"operations/#torchsqrt","text":"torch.sqrt(a) \u2192 Tensor Returns element-wise square root of the tensor.","title":"torch.sqrt"},{"location":"operations/#parameters_13","text":"a (Tensor) - Input Tensor.","title":"Parameters"},{"location":"operations/#example_13","text":">>> let a = torch.tensor([[1,9], [4,16]]); >>> let b = torch.sqrt(a); >>> b.data; // [[1,3], // [2,4]] Note: torch.sqrt(a) is the same as a.sqrt() .","title":"Example"},{"location":"operations/#torchexp","text":"torch.exp(a) \u2192 Tensor Returns element-wise exponentiation of the tensor.","title":"torch.exp"},{"location":"operations/#parameters_14","text":"a (Tensor) - Input Tensor.","title":"Parameters"},{"location":"operations/#example_14","text":">>> let a = torch.tensor([[1,2], [0,-1]]); >>> let b = torch.exp(a); >>> b.data; // [[2.71828,7.389056], // [1.00000,0.36788]] Note: torch.exp(a) is the same as a.exp() .","title":"Example"},{"location":"operations/#torchlog","text":"torch.log(a) \u2192 Tensor Returns element-wise natural log of the tensor.","title":"torch.log"},{"location":"operations/#parameters_15","text":"a (Tensor) - Input Tensor.","title":"Parameters"},{"location":"operations/#example_15","text":">>> let a = torch.tensor([[1,2], [0.01,3]]); >>> let b = torch.log(a); >>> b.data; // [[0.00000,0.693147], // [-4.6051,1.098612]] Note: torch.log(a) is the same as a.log() .","title":"Example"},{"location":"tensor/","text":"Tensor The Tensor is the main object of this library. In this section are listed all of the Tensor methods. torch.tensor torch.tensor(data, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with the data in the argument data . Parameters data (Array) - Javascript Array containing the data to be stored in the Tensor. This array can have any number of dimensions, but must have a homogenous shape, and only numbers. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it. Example >>> let a = torch.tensor([[1,2,3],[4,5,6]], false, 'gpu'); >>> console.log(a.data); //[[1,2,3], // [4,5,6]] torch.zeros torch.zeros(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with zeros with dimensions like shape . Parameters shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it. Example >>> let a = torch.zeros([3,2], false, 'gpu'); >>> console.log(a.data); //[[0, 0], // [0, 0] // [0, 0]] torch.ones torch.ones(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with ones with dimensions like shape . Parameters shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it. Example >>> let a = torch.ones([3,2], false, 'gpu'); >>> console.log(a.data); //[[1, 1], // [1, 1] // [1, 1]] torch.tril torch.tril(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a 2D lower triangular tensor. Parameters shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it. Example >>> let a = torch.tril([4,3], false, 'gpu'); >>> console.log(a.data); //[[1, 0, 0], // [1, 1, 0] // [1, 1, 1] // [1, 1, 1]] >>> let b = torch.tril([3,4], false, 'gpu'); >>> console.log(b.data); //[[1, 0, 0, 0], // [1, 1, 0, 0] // [1, 1, 1, 0] torch.randn torch.randn(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with randomly sampled data with dimensions like shape . The sample is from a normal distribution. Parameters shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it. Example >>> let a = torch.randn([3,2], false, 'gpu'); >>> console.log(a.data); //[[1.001, 0.122], // [-0.93, 0.125] // [0.123,-0.001]] torch.rand torch.rand(*shape, requires_grad=false, device='cpu') \u2192 Tensor Creates new instance of the Tensor class filled with numbers in a uniform distribution in ]0,1[. Parameters shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it. Example >>> let a = torch.rand([3,2], false, 'gpu'); >>> console.log(a.data); //[[0.011, 0.122], // [-0.03, 0.105] // [-0.90,-0.202]] torch.randint torch.rand(low, high, *shape, requires_grad=false, device='cpu') \u2192 Tensor Creates new instance of the Tensor class filled with random integers between low and high . Parameters low - Lowest integer that can be sampled. high - One above highest integer that can be sampled. shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it. Example >>> let a = torch.rand([3,2], false, 'gpu'); >>> console.log(a.data); //[[0.011, 0.122], // [-0.03, 0.105] // [-0.90,-0.202]] tensor.backward Performs backpropagation from this tensor backwards. It fills the gradients of every tensor that led to this one with gradients relative to this tensor. Note: This only happens to tensors that have requires_grad set to true. Example >>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], false, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]] tensor.zero_grad Clears the gradients stored in this tensor. Sets the gadients to zero. This is used after the gradients have been used to update the parameters of a model, to set the model up for the next iteration. Example >>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], false, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]] >>> a.zero_grad(); >>> console.log(a.grad); //[[0, 0], // [0, 0] // [0, 0]] tensor.zero_grad_graph Clears the gradients stored in this tensor, setting the gadients to zero, and does the same for every tensor that led to this one. This is used after the gradients have been used to update the parameters of a model, to set the model up for the next iteration. Example >>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], true, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]] >>> c.zero_grad_graph(); // Clears gradients of c, b, and a. >>> console.log(a.grad); //[[0, 0], // [0, 0] // [0, 0]] tensor.tolist Returns an Array with the tensor's data. Example >>> let a = torch.randn([3,2], true, 'gpu'); >>> let aArray = a.tolist(); >>> console.log(aArray); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]] tensor.data Returns the tensor's data as a javascript Array. Example >>> let a = torch.randn([3,2], true, 'gpu'); >>> console.log(a.data); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]] tensor.length Returns the tensor's length (size of first dimension). Example >>> let a = torch.randn([3,2], true, 'gpu'); >>> console.log(a.length); // 3 tensor.ndims Returns the number of dimensions in the Tensor. Example >>> let a = torch.randn([3,2,1,4], true, 'gpu'); >>> console.log(a.ndims); // 4 tensor.grad Returns the gradients currently stored in the Tensor. Example >>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], true, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]]","title":"Tensor"},{"location":"tensor/#tensor","text":"The Tensor is the main object of this library. In this section are listed all of the Tensor methods.","title":"Tensor"},{"location":"tensor/#torchtensor","text":"torch.tensor(data, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with the data in the argument data .","title":"torch.tensor"},{"location":"tensor/#parameters","text":"data (Array) - Javascript Array containing the data to be stored in the Tensor. This array can have any number of dimensions, but must have a homogenous shape, and only numbers. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.","title":"Parameters"},{"location":"tensor/#example","text":">>> let a = torch.tensor([[1,2,3],[4,5,6]], false, 'gpu'); >>> console.log(a.data); //[[1,2,3], // [4,5,6]]","title":"Example"},{"location":"tensor/#torchzeros","text":"torch.zeros(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with zeros with dimensions like shape .","title":"torch.zeros"},{"location":"tensor/#parameters_1","text":"shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.","title":"Parameters"},{"location":"tensor/#example_1","text":">>> let a = torch.zeros([3,2], false, 'gpu'); >>> console.log(a.data); //[[0, 0], // [0, 0] // [0, 0]]","title":"Example"},{"location":"tensor/#torchones","text":"torch.ones(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with ones with dimensions like shape .","title":"torch.ones"},{"location":"tensor/#parameters_2","text":"shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.","title":"Parameters"},{"location":"tensor/#example_2","text":">>> let a = torch.ones([3,2], false, 'gpu'); >>> console.log(a.data); //[[1, 1], // [1, 1] // [1, 1]]","title":"Example"},{"location":"tensor/#torchtril","text":"torch.tril(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a 2D lower triangular tensor.","title":"torch.tril"},{"location":"tensor/#parameters_3","text":"shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.","title":"Parameters"},{"location":"tensor/#example_3","text":">>> let a = torch.tril([4,3], false, 'gpu'); >>> console.log(a.data); //[[1, 0, 0], // [1, 1, 0] // [1, 1, 1] // [1, 1, 1]] >>> let b = torch.tril([3,4], false, 'gpu'); >>> console.log(b.data); //[[1, 0, 0, 0], // [1, 1, 0, 0] // [1, 1, 1, 0]","title":"Example"},{"location":"tensor/#torchrandn","text":"torch.randn(*shape, requires_grad=false, device='cpu') \u2192 Tensor Returns a tensor filled with randomly sampled data with dimensions like shape . The sample is from a normal distribution.","title":"torch.randn"},{"location":"tensor/#parameters_4","text":"shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.","title":"Parameters"},{"location":"tensor/#example_4","text":">>> let a = torch.randn([3,2], false, 'gpu'); >>> console.log(a.data); //[[1.001, 0.122], // [-0.93, 0.125] // [0.123,-0.001]]","title":"Example"},{"location":"tensor/#torchrand","text":"torch.rand(*shape, requires_grad=false, device='cpu') \u2192 Tensor Creates new instance of the Tensor class filled with numbers in a uniform distribution in ]0,1[.","title":"torch.rand"},{"location":"tensor/#parameters_5","text":"shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.","title":"Parameters"},{"location":"tensor/#example_5","text":">>> let a = torch.rand([3,2], false, 'gpu'); >>> console.log(a.data); //[[0.011, 0.122], // [-0.03, 0.105] // [-0.90,-0.202]]","title":"Example"},{"location":"tensor/#torchrandint","text":"torch.rand(low, high, *shape, requires_grad=false, device='cpu') \u2192 Tensor Creates new instance of the Tensor class filled with random integers between low and high .","title":"torch.randint"},{"location":"tensor/#parameters_6","text":"low - Lowest integer that can be sampled. high - One above highest integer that can be sampled. shape (Array) - Javascript Array containing the shape of the Tensor. requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: false . device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.","title":"Parameters"},{"location":"tensor/#example_6","text":">>> let a = torch.rand([3,2], false, 'gpu'); >>> console.log(a.data); //[[0.011, 0.122], // [-0.03, 0.105] // [-0.90,-0.202]]","title":"Example"},{"location":"tensor/#tensorbackward","text":"Performs backpropagation from this tensor backwards. It fills the gradients of every tensor that led to this one with gradients relative to this tensor. Note: This only happens to tensors that have requires_grad set to true.","title":"tensor.backward"},{"location":"tensor/#example_7","text":">>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], false, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]]","title":"Example"},{"location":"tensor/#tensorzero_grad","text":"Clears the gradients stored in this tensor. Sets the gadients to zero. This is used after the gradients have been used to update the parameters of a model, to set the model up for the next iteration.","title":"tensor.zero_grad"},{"location":"tensor/#example_8","text":">>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], false, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]] >>> a.zero_grad(); >>> console.log(a.grad); //[[0, 0], // [0, 0] // [0, 0]]","title":"Example"},{"location":"tensor/#tensorzero_grad_graph","text":"Clears the gradients stored in this tensor, setting the gadients to zero, and does the same for every tensor that led to this one. This is used after the gradients have been used to update the parameters of a model, to set the model up for the next iteration.","title":"tensor.zero_grad_graph"},{"location":"tensor/#example_9","text":">>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], true, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]] >>> c.zero_grad_graph(); // Clears gradients of c, b, and a. >>> console.log(a.grad); //[[0, 0], // [0, 0] // [0, 0]]","title":"Example"},{"location":"tensor/#tensortolist","text":"Returns an Array with the tensor's data.","title":"tensor.tolist"},{"location":"tensor/#example_10","text":">>> let a = torch.randn([3,2], true, 'gpu'); >>> let aArray = a.tolist(); >>> console.log(aArray); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]]","title":"Example"},{"location":"tensor/#tensordata","text":"Returns the tensor's data as a javascript Array.","title":"tensor.data"},{"location":"tensor/#example_11","text":">>> let a = torch.randn([3,2], true, 'gpu'); >>> console.log(a.data); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]]","title":"Example"},{"location":"tensor/#tensorlength","text":"Returns the tensor's length (size of first dimension).","title":"tensor.length"},{"location":"tensor/#example_12","text":">>> let a = torch.randn([3,2], true, 'gpu'); >>> console.log(a.length); // 3","title":"Example"},{"location":"tensor/#tensorndims","text":"Returns the number of dimensions in the Tensor.","title":"tensor.ndims"},{"location":"tensor/#example_13","text":">>> let a = torch.randn([3,2,1,4], true, 'gpu'); >>> console.log(a.ndims); // 4","title":"Example"},{"location":"tensor/#tensorgrad","text":"Returns the gradients currently stored in the Tensor.","title":"tensor.grad"},{"location":"tensor/#example_14","text":">>> let a = torch.randn([3,2], true, 'gpu'); >>> let b = torch.randn([2,4], true, 'gpu'); >>> let c = torch.matmul(a,b); >>> c.backward(); >>> console.log(a.grad); //[[0.001, -0.99], // [-0.13, 0.333] // [-0.91,-0.044]]","title":"Example"},{"location":"tutorials/","text":"Tutorials This section contains ready-to-use examples of JS-PyTorch in action, with increasing complexity and explainations along the way. Gradients To use the autograd functionality (get Tensor's gradients), first create your input tensor, and your parameter tensors. We want to see the gradients of the parameter tensors relative to the output, so we set requires_grad=true on them. const { torch } = require(\"js-pytorch\"); // Instantiate Input Tensor: let x = torch.randn([8, 4, 5], false); // Instantiate Parameter Tensors: let w = torch.randn([8, 5, 4], true); let b = torch.tensor([0.2, 0.5, 0.1, 0.0], true); The parameter tensor w will be multiplied by the input tensor x . The parameter tensor b will be added to the input tensor x . // Make calculations: let y = torch.matmul(x, w); y = torch.add(out, b); Now, compute the gradients of w and b using tensor.backward on the output tensor y . // Compute gradients on whole graph: y.backward(); // Get gradients from specific Tensors: console.log(w.grad); console.log(b.grad); To access the gradients of a tensor after calling tensor.backward on the output, use the syntax tensor.grad . Neural Network To train a neural network from scratch, first import the torch , torch.nn and torch.optim modules. const torch = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; Now, create the Neural Network class. This class extends the nn.Module object. In the constructor, add the layers that make up your model, adding them as attributes of your Neural Network class. In this case, there is: A Linear layer, stored in this.w1 . A ReLU activation function, stored in this.relu1 . A Linear layer, stored in this.w2 . A ReLU activation function, stored in this.relu2 . A Linear layer, stored in this.w3 . The size of these layers depends on three parameters, passed into the constructor: input_size , defines the size of the last dimension of the input tensor. hidden_size , defines the size of each hidden layer in the model. out_size defines the size of the last dimension of the output tensor. This is the same as the number of classes of the output. Note: The input_size must be the input size of the first layer, and out_size must be the output size of the last layer. After the constructor, create a forward method, where an input is passed through each layer in the model. To pass the input through a layer, use: this.layer.forward(input); The final class is as follows: // Implement Module class: class NeuralNet extends nn.Module { constructor(in_size, hidden_size, out_size) { super(); // Instantiate Neural Network's Layers: this.w1 = new nn.Linear(in_size, hidden_size); this.relu1 = new nn.ReLU(); this.w2 = new nn.Linear(hidden_size, hidden_size); this.relu2 = new nn.ReLU(); this.w3 = new nn.Linear(hidden_size, out_size); }; forward(x) { let z; z = this.w1.forward(x); z = this.relu1.forward(z); z = this.w2.forward(z); z = this.relu2.forward(z); z = this.w3.forward(z); return z; }; }; Note: To add the module to the gpu for faster computation, pass the argument 'gpu' to the Linear layers. Now, create an instance of your NeuralNetwork class. Declare the in_size , hidden_size and out_size according to your data, and fine a batch size: // Instantiate Model: let in_size = 16; let hidden_size = 32; let out_size = 10; let batch_size = 16; let model = new NeuralNet(in_size,hidden_size,out_size); Instantiate the loss function and optimizer, passing the parameters of the models using the model.parameters() method and the learning rate. // Define loss function and optimizer: let loss_func = new nn.CrossEntropyLoss(); let optimizer = new optim.Adam(model.parameters(), 3e-3); Import the data, and add it to x (input) and y (target) variables. Note: Here, we are generating a dummy dataset (random input and target). // Instantiate input and output: let x = torch.randn([batch_size, in_size]); let y = torch.randint(0, out_size, [batch_size]); let loss; Create a train loop to train your model: // Training Loop: for (let i = 0; i < 256; i++) { let z = model.forward(x); // Get loss: loss = loss_func.forward(z, y); // Backpropagate the loss using torch.tensor's backward() method: loss.backward(); // Update the weights: optimizer.step(); // Reset the gradients to zero after each training step: optimizer.zero_grad(); // Print current loss: console.log(`Iter: ${i} - Loss: ${loss.data}`); } Detailing On each pass through the training loop, the following happens: - The input is passed through the model: let z = model.forward(x); - The loss is calculated: loss = loss_func.forward(z, y); - The gradients are computed: loss.backward(); - The parameters are optimized: optimizer.step(); - The gradients are reset: optimizer.zero_grad(); - The current loss is printed to the console: console.log(`Iter: ${i} - Loss: ${loss.data}`); Now, the entire Neural Network, with: Class declaration. Hyperparameter Definition. Train Loop. Full Implementation const torch = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; // Implement Module class: class NeuralNet extends nn.Module { constructor(in_size, hidden_size, out_size) { super(); // Instantiate Neural Network's Layers: this.w1 = new nn.Linear(in_size, hidden_size); this.relu1 = new nn.ReLU(); this.w2 = new nn.Linear(hidden_size, hidden_size); this.relu2 = new nn.ReLU(); this.w3 = new nn.Linear(hidden_size, out_size); }; forward(x) { let z; z = this.w1.forward(x); z = this.relu1.forward(z); z = this.w2.forward(z); z = this.relu2.forward(z); z = this.w3.forward(z); return z; }; }; // Instantiate Model: let in_size = 16; let hidden_size = 32; let out_size = 10; let batch_size = 16; let model = new NeuralNet(in_size,hidden_size,out_size); // Define loss function and optimizer: let loss_func = new nn.CrossEntropyLoss(); let optimizer = new optim.Adam(model.parameters(), 3e-3); // Instantiate input and output: let x = torch.randn([batch_size, in_size]); let y = torch.randint(0, out_size, [batch_size]); let loss; // Training Loop: for (let i = 0; i < 256; i++) { let z = model.forward(x); // Get loss: loss = loss_func.forward(z, y); // Backpropagate the loss using torch.tensor's backward() method: loss.backward(); // Update the weights: optimizer.step(); // Reset the gradients to zero after each training step: optimizer.zero_grad(); // Print current loss: console.log(`Iter: ${i} - Loss: ${loss.data}`); } Transformer Following the exact same steps as the last tutorial, we can create a Transformer Model . The inputs of the constructor are: vocab_size , defines the size of the last dimension of the input and output tensor. It is the number of characters or words that compose your vocabulary. hidden_size , defines the size of each hidden layer in the model. n_timesteps number of timesteps computed in parallel by the transformer. dropout_p probability of randomly dropping an activation during training (to improve regularization). device is the device on which the model's calculations will run. Either 'cpu' or 'gpu' . Full Implementation const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; const device = 'gpu'; // Create Transformer decoder Module: class Transformer extends nn.Module { constructor(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p, device) { super(); // Instantiate Transformer's Layers: this.embed = new nn.Embedding(vocab_size, hidden_size); this.pos_embed = new nn.PositionalEmbedding(n_timesteps, hidden_size); this.b1 = new nn.Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device); this.b2 = new nn.Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device); this.ln = new nn.LayerNorm(hidden_size); this.linear = new nn.Linear(hidden_size, vocab_size, device); } forward(x) { let z; z = torch.add(this.embed.forward(x), this.pos_embed.forward(x)); z = this.b1.forward(z); z = this.b2.forward(z); z = this.ln.forward(z); z = this.linear.forward(z); return z; } } // Define training hyperparameters: const vocab_size = 52; const hidden_size = 32; const n_timesteps = 16; const n_heads = 4; const dropout_p = 0; const batch_size = 8; // Instantiate your custom nn.Module: const model = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p, device); // Define loss function and optimizer: const loss_func = new nn.CrossEntropyLoss(); const optimizer = new optim.Adam(model.parameters(), (lr = 5e-3), (reg = 0)); // Instantiate sample input and output: let x = torch.randint(0, vocab_size, [batch_size, n_timesteps, 1]); let y = torch.randint(0, vocab_size, [batch_size, n_timesteps]); let loss; // Training Loop: for (let i = 0; i < 256; i++) { // Forward pass through the Transformer: let z = model.forward(x); // Get loss: loss = loss_func.forward(z, y); // Backpropagate the loss using torch.tensor's backward() method: loss.backward(); // Update the weights: optimizer.step(); // Reset the gradients to zero after each training step: optimizer.zero_grad(); // Print loss at every iteration: console.log(`Iter ${i} - Loss ${loss.data[0].toFixed(4)}`) } Saving and Loading Models To save a model, first instantiate a class extending nn.Module for your model, as explained in the previous tutorials. // Instantiate your model: const model = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p); Then, train your model. When you are finished, or during training (to generate snapshots), save the model to a JSON file using torch.save() : // Save model to JSON file: torch.save(model, 'model.json') To load the model, instantiate a placeholder as an empty instance of the same model: // To load, instantiate placeHolder using the original model's architecture: const placeHolder = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p); Then, load the weights of the trained model into the placeholder using torch.load() : // Load weights into placeHolder: const newModel = torch.load(placeHolder, 'model.json') Testing To test a model, just run your test data through the trained model using model.forward() : // Load weights into placeHolder: let z = model.forward(x); Then, use a loss function or a custom function to calculate your loss or accuracy in comparaison with the target : let loss = nn.CrossEntropyLoss(z,y);","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"This section contains ready-to-use examples of JS-PyTorch in action, with increasing complexity and explainations along the way.","title":"Tutorials"},{"location":"tutorials/#gradients","text":"To use the autograd functionality (get Tensor's gradients), first create your input tensor, and your parameter tensors. We want to see the gradients of the parameter tensors relative to the output, so we set requires_grad=true on them. const { torch } = require(\"js-pytorch\"); // Instantiate Input Tensor: let x = torch.randn([8, 4, 5], false); // Instantiate Parameter Tensors: let w = torch.randn([8, 5, 4], true); let b = torch.tensor([0.2, 0.5, 0.1, 0.0], true); The parameter tensor w will be multiplied by the input tensor x . The parameter tensor b will be added to the input tensor x . // Make calculations: let y = torch.matmul(x, w); y = torch.add(out, b); Now, compute the gradients of w and b using tensor.backward on the output tensor y . // Compute gradients on whole graph: y.backward(); // Get gradients from specific Tensors: console.log(w.grad); console.log(b.grad); To access the gradients of a tensor after calling tensor.backward on the output, use the syntax tensor.grad .","title":"Gradients"},{"location":"tutorials/#neural-network","text":"To train a neural network from scratch, first import the torch , torch.nn and torch.optim modules. const torch = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; Now, create the Neural Network class. This class extends the nn.Module object. In the constructor, add the layers that make up your model, adding them as attributes of your Neural Network class. In this case, there is: A Linear layer, stored in this.w1 . A ReLU activation function, stored in this.relu1 . A Linear layer, stored in this.w2 . A ReLU activation function, stored in this.relu2 . A Linear layer, stored in this.w3 . The size of these layers depends on three parameters, passed into the constructor: input_size , defines the size of the last dimension of the input tensor. hidden_size , defines the size of each hidden layer in the model. out_size defines the size of the last dimension of the output tensor. This is the same as the number of classes of the output. Note: The input_size must be the input size of the first layer, and out_size must be the output size of the last layer. After the constructor, create a forward method, where an input is passed through each layer in the model. To pass the input through a layer, use: this.layer.forward(input); The final class is as follows: // Implement Module class: class NeuralNet extends nn.Module { constructor(in_size, hidden_size, out_size) { super(); // Instantiate Neural Network's Layers: this.w1 = new nn.Linear(in_size, hidden_size); this.relu1 = new nn.ReLU(); this.w2 = new nn.Linear(hidden_size, hidden_size); this.relu2 = new nn.ReLU(); this.w3 = new nn.Linear(hidden_size, out_size); }; forward(x) { let z; z = this.w1.forward(x); z = this.relu1.forward(z); z = this.w2.forward(z); z = this.relu2.forward(z); z = this.w3.forward(z); return z; }; }; Note: To add the module to the gpu for faster computation, pass the argument 'gpu' to the Linear layers. Now, create an instance of your NeuralNetwork class. Declare the in_size , hidden_size and out_size according to your data, and fine a batch size: // Instantiate Model: let in_size = 16; let hidden_size = 32; let out_size = 10; let batch_size = 16; let model = new NeuralNet(in_size,hidden_size,out_size); Instantiate the loss function and optimizer, passing the parameters of the models using the model.parameters() method and the learning rate. // Define loss function and optimizer: let loss_func = new nn.CrossEntropyLoss(); let optimizer = new optim.Adam(model.parameters(), 3e-3); Import the data, and add it to x (input) and y (target) variables. Note: Here, we are generating a dummy dataset (random input and target). // Instantiate input and output: let x = torch.randn([batch_size, in_size]); let y = torch.randint(0, out_size, [batch_size]); let loss; Create a train loop to train your model: // Training Loop: for (let i = 0; i < 256; i++) { let z = model.forward(x); // Get loss: loss = loss_func.forward(z, y); // Backpropagate the loss using torch.tensor's backward() method: loss.backward(); // Update the weights: optimizer.step(); // Reset the gradients to zero after each training step: optimizer.zero_grad(); // Print current loss: console.log(`Iter: ${i} - Loss: ${loss.data}`); } Detailing On each pass through the training loop, the following happens: - The input is passed through the model: let z = model.forward(x); - The loss is calculated: loss = loss_func.forward(z, y); - The gradients are computed: loss.backward(); - The parameters are optimized: optimizer.step(); - The gradients are reset: optimizer.zero_grad(); - The current loss is printed to the console: console.log(`Iter: ${i} - Loss: ${loss.data}`); Now, the entire Neural Network, with: Class declaration. Hyperparameter Definition. Train Loop. Full Implementation const torch = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; // Implement Module class: class NeuralNet extends nn.Module { constructor(in_size, hidden_size, out_size) { super(); // Instantiate Neural Network's Layers: this.w1 = new nn.Linear(in_size, hidden_size); this.relu1 = new nn.ReLU(); this.w2 = new nn.Linear(hidden_size, hidden_size); this.relu2 = new nn.ReLU(); this.w3 = new nn.Linear(hidden_size, out_size); }; forward(x) { let z; z = this.w1.forward(x); z = this.relu1.forward(z); z = this.w2.forward(z); z = this.relu2.forward(z); z = this.w3.forward(z); return z; }; }; // Instantiate Model: let in_size = 16; let hidden_size = 32; let out_size = 10; let batch_size = 16; let model = new NeuralNet(in_size,hidden_size,out_size); // Define loss function and optimizer: let loss_func = new nn.CrossEntropyLoss(); let optimizer = new optim.Adam(model.parameters(), 3e-3); // Instantiate input and output: let x = torch.randn([batch_size, in_size]); let y = torch.randint(0, out_size, [batch_size]); let loss; // Training Loop: for (let i = 0; i < 256; i++) { let z = model.forward(x); // Get loss: loss = loss_func.forward(z, y); // Backpropagate the loss using torch.tensor's backward() method: loss.backward(); // Update the weights: optimizer.step(); // Reset the gradients to zero after each training step: optimizer.zero_grad(); // Print current loss: console.log(`Iter: ${i} - Loss: ${loss.data}`); }","title":"Neural Network"},{"location":"tutorials/#transformer","text":"Following the exact same steps as the last tutorial, we can create a Transformer Model . The inputs of the constructor are: vocab_size , defines the size of the last dimension of the input and output tensor. It is the number of characters or words that compose your vocabulary. hidden_size , defines the size of each hidden layer in the model. n_timesteps number of timesteps computed in parallel by the transformer. dropout_p probability of randomly dropping an activation during training (to improve regularization). device is the device on which the model's calculations will run. Either 'cpu' or 'gpu' . Full Implementation const { torch } = require(\"js-pytorch\"); const nn = torch.nn; const optim = torch.optim; const device = 'gpu'; // Create Transformer decoder Module: class Transformer extends nn.Module { constructor(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p, device) { super(); // Instantiate Transformer's Layers: this.embed = new nn.Embedding(vocab_size, hidden_size); this.pos_embed = new nn.PositionalEmbedding(n_timesteps, hidden_size); this.b1 = new nn.Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device); this.b2 = new nn.Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device); this.ln = new nn.LayerNorm(hidden_size); this.linear = new nn.Linear(hidden_size, vocab_size, device); } forward(x) { let z; z = torch.add(this.embed.forward(x), this.pos_embed.forward(x)); z = this.b1.forward(z); z = this.b2.forward(z); z = this.ln.forward(z); z = this.linear.forward(z); return z; } } // Define training hyperparameters: const vocab_size = 52; const hidden_size = 32; const n_timesteps = 16; const n_heads = 4; const dropout_p = 0; const batch_size = 8; // Instantiate your custom nn.Module: const model = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p, device); // Define loss function and optimizer: const loss_func = new nn.CrossEntropyLoss(); const optimizer = new optim.Adam(model.parameters(), (lr = 5e-3), (reg = 0)); // Instantiate sample input and output: let x = torch.randint(0, vocab_size, [batch_size, n_timesteps, 1]); let y = torch.randint(0, vocab_size, [batch_size, n_timesteps]); let loss; // Training Loop: for (let i = 0; i < 256; i++) { // Forward pass through the Transformer: let z = model.forward(x); // Get loss: loss = loss_func.forward(z, y); // Backpropagate the loss using torch.tensor's backward() method: loss.backward(); // Update the weights: optimizer.step(); // Reset the gradients to zero after each training step: optimizer.zero_grad(); // Print loss at every iteration: console.log(`Iter ${i} - Loss ${loss.data[0].toFixed(4)}`) }","title":"Transformer"},{"location":"tutorials/#saving-and-loading-models","text":"To save a model, first instantiate a class extending nn.Module for your model, as explained in the previous tutorials. // Instantiate your model: const model = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p); Then, train your model. When you are finished, or during training (to generate snapshots), save the model to a JSON file using torch.save() : // Save model to JSON file: torch.save(model, 'model.json') To load the model, instantiate a placeholder as an empty instance of the same model: // To load, instantiate placeHolder using the original model's architecture: const placeHolder = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p); Then, load the weights of the trained model into the placeholder using torch.load() : // Load weights into placeHolder: const newModel = torch.load(placeHolder, 'model.json')","title":"Saving and Loading Models"},{"location":"tutorials/#testing","text":"To test a model, just run your test data through the trained model using model.forward() : // Load weights into placeHolder: let z = model.forward(x); Then, use a loss function or a custom function to calculate your loss or accuracy in comparaison with the target : let loss = nn.CrossEntropyLoss(z,y);","title":"Testing"}]}